{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5500782b",
   "metadata": {},
   "source": [
    "Here's a **detailed and structured note** covering every concept from the lecture on **PCA in Higher Dimensions** from your course (Lecture 6, Week 6). The goal is to understand how **PCA can be implemented efficiently** when **dimensionality (d)** is much greater than the **number of data points (n)**.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ“˜ PCA in Higher Dimensions â€” Detailed Notes\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”· Objective\n",
    "\n",
    "To **perform PCA efficiently** when:\n",
    "\n",
    "* The data is high-dimensional (i.e., the feature space dimension `d` is very large).\n",
    "* The number of data points `n` is **small** (i.e., `d >> n`).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¶ Motivation\n",
    "\n",
    "In standard PCA, we compute eigenvectors of the **covariance matrix** $C \\in \\mathbb{R}^{d \\times d}$.\n",
    "But when **d is large**, this is computationally **expensive**.\n",
    "\n",
    "The trick:\n",
    "Reformulate the PCA problem to work with an **$n \\times n$** matrix (much smaller), which is computationally easier.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¶ PCA Recap\n",
    "\n",
    "Given:\n",
    "\n",
    "* $\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_n \\in \\mathbb{R}^d$\n",
    "* Mean of data:\n",
    "\n",
    "  $$\n",
    "  \\bar{\\mathbf{x}} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{x}_i\n",
    "  $$\n",
    "\n",
    "Define:\n",
    "\n",
    "* Mean-centered data:\n",
    "\n",
    "  $$\n",
    "  \\mathbf{A} = [\\mathbf{x}_1 - \\bar{\\mathbf{x}}, \\dots, \\mathbf{x}_n - \\bar{\\mathbf{x}}] \\in \\mathbb{R}^{d \\times n}\n",
    "  $$\n",
    "\n",
    "Covariance matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\frac{1}{n} \\sum_{i=1}^n (\\mathbf{x}_i - \\bar{\\mathbf{x}})(\\mathbf{x}_i - \\bar{\\mathbf{x}})^T = \\frac{1}{n} \\mathbf{A} \\mathbf{A}^T \\in \\mathbb{R}^{d \\times d}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¶ Problem with Naive PCA in High Dimensions\n",
    "\n",
    "* $\\mathbf{C} = \\frac{1}{n} \\mathbf{A} \\mathbf{A}^T$ is a **$d \\times d$** matrix.\n",
    "* If **$d \\gg n$**, computing eigenvectors/eigenvalues of $\\mathbf{C}$ is **computationally expensive**.\n",
    "\n",
    "**Key Insight**:\n",
    "The **rank** of $\\mathbf{C}$ is at most $n$, because it is a sum of $n$ rank-one matrices:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\frac{1}{n} \\sum_{i=1}^n (\\mathbf{x}_i - \\bar{\\mathbf{x}})(\\mathbf{x}_i - \\bar{\\mathbf{x}})^T\n",
    "$$\n",
    "\n",
    "* Each term is rank 1 â‡’ max rank of sum is $n$\n",
    "* So, **$d - n$ eigenvalues are zero**\n",
    "* Itâ€™s wasteful to compute $d$ eigenvectors when only $n$ can be nonzero.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¶ Solution: Work with an $n \\times n$ Matrix Instead\n",
    "\n",
    "### Step 1: Define matrix A\n",
    "\n",
    "Let:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = [\\mathbf{x}_1 - \\bar{\\mathbf{x}}, \\dots, \\mathbf{x}_n - \\bar{\\mathbf{x}}] \\in \\mathbb{R}^{d \\times n}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\frac{1}{n} \\mathbf{A} \\mathbf{A}^T \\in \\mathbb{R}^{d \\times d}\n",
    "$$\n",
    "\n",
    "Letâ€™s instead consider:\n",
    "\n",
    "$$\n",
    "\\mathbf{C}' = \\frac{1}{n} \\mathbf{A}^T \\mathbf{A} \\in \\mathbb{R}^{n \\times n}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¶ Eigenvalue Trick\n",
    "\n",
    "Let:\n",
    "\n",
    "* $\\lambda_i$ be a non-zero eigenvalue of $\\mathbf{C} = \\frac{1}{n} \\mathbf{A} \\mathbf{A}^T$\n",
    "* $\\mathbf{u}_i \\in \\mathbb{R}^d$ be the corresponding eigenvector\n",
    "\n",
    "We claim:\n",
    "\n",
    "$$\n",
    "\\lambda_i \\text{ is also an eigenvalue of } \\frac{1}{n} \\mathbf{A}^T \\mathbf{A}, \\text{ and vice versa}\n",
    "$$\n",
    "\n",
    "### âœ… Proof Sketch:\n",
    "\n",
    "Assume:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} \\mathbf{u}_i = \\lambda_i \\mathbf{u}_i \\Rightarrow \\frac{1}{n} \\mathbf{A} \\mathbf{A}^T \\mathbf{u}_i = \\lambda_i \\mathbf{u}_i\n",
    "$$\n",
    "\n",
    "Multiply both sides by $\\mathbf{A}^T$:\n",
    "\n",
    "$$\n",
    "\\Rightarrow \\frac{1}{n} \\mathbf{A}^T \\mathbf{A} \\mathbf{A}^T \\mathbf{u}_i = \\lambda_i \\mathbf{A}^T \\mathbf{u}_i\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^T \\mathbf{u}_i \\text{ is an eigenvector of } \\mathbf{A}^T \\mathbf{A}\n",
    "$$\n",
    "\n",
    "Hence:\n",
    "\n",
    "* If $\\mathbf{v}_i$ is an eigenvector of $\\frac{1}{n} \\mathbf{A}^T \\mathbf{A}$, then\n",
    "\n",
    "  $$\n",
    "  \\mathbf{u}_i = \\frac{1}{\\sqrt{\\lambda_i}} \\mathbf{A} \\mathbf{v}_i\n",
    "  $$\n",
    "\n",
    "  is an eigenvector of $\\frac{1}{n} \\mathbf{A} \\mathbf{A}^T$\n",
    "\n",
    "This lets us **construct the principal components $\\mathbf{u}_i$** from $\\mathbf{v}_i$!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¶ Final Procedure: PCA in High Dimensions\n",
    "\n",
    "1. Compute **mean-centered data matrix** $\\mathbf{A} \\in \\mathbb{R}^{d \\times n}$\n",
    "\n",
    "2. Form $\\mathbf{C}' = \\frac{1}{n} \\mathbf{A}^T \\mathbf{A} \\in \\mathbb{R}^{n \\times n}$\n",
    "\n",
    "3. Find the **top k eigenvectors** $\\mathbf{v}_1, \\dots, \\mathbf{v}_k$ of $\\mathbf{C}'$\n",
    "\n",
    "4. Compute the **top k PCA directions** in $\\mathbb{R}^d$ as:\n",
    "\n",
    "   $$\n",
    "   \\mathbf{u}_i = \\frac{1}{\\sqrt{\\lambda_i}} \\mathbf{A} \\mathbf{v}_i, \\quad i = 1, \\dots, k\n",
    "   $$\n",
    "\n",
    "5. These $\\mathbf{u}_i$'s are the **principal components** (columns of the projection matrix)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¶ Key Learning Outcomes\n",
    "\n",
    "âœ… Reformulate the PCA eigenvector computation from $d \\times d$ to $n \\times n$ matrix\n",
    "\n",
    "âœ… Use the **duality between** $\\mathbf{A}^T \\mathbf{A}$ and $\\mathbf{A} \\mathbf{A}^T$ for eigenvalue problems\n",
    "\n",
    "âœ… Achieve **exact same PCA projection** but with **lower computational cost**\n",
    "\n",
    "âœ… Especially useful when **features (d) are huge** but **samples (n) are few**\n",
    "\n",
    "âœ… Connection to **SVD**:\n",
    "\n",
    "* This trick is the basis of PCA via **Singular Value Decomposition**:\n",
    "\n",
    "  $$\n",
    "  \\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n",
    "  $$\n",
    "\n",
    "  * Eigenvectors of $\\mathbf{C} = \\mathbf{A} \\mathbf{A}^T$ are columns of $\\mathbf{U}$\n",
    "  * Eigenvectors of $\\mathbf{A}^T \\mathbf{A}$ are columns of $\\mathbf{V}$\n",
    "  * Singular values are $\\sigma_i = \\sqrt{\\lambda_i}$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”š Summary\n",
    "\n",
    "* **Goal**: Perform PCA efficiently in high-dimensional space\n",
    "* **Strategy**: Work with $n \\times n$ matrix $\\mathbf{A}^T \\mathbf{A}$ instead of $d \\times d$ covariance matrix\n",
    "* **Benefit**: Reduce computation from $O(d^3)$ to $O(n^3)$ when $d \\gg n$\n",
    "* **Application**: Widely used in text, genomics, and image processing where features are more than samples"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
