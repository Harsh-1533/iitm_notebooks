{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e12c989",
   "metadata": {},
   "source": [
    "Here's a detailed, in-depth breakdown of the **\"Principal Component Analysis (PCA) ‚Äì Lecture 4 (Contd.)\"** covering each concept mentioned in the lecture. This is designed as comprehensive study notes:\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Principal Component Analysis (PCA) ‚Äì Lecture 4 (Contd.)\n",
    "\n",
    "### üìò Lecture Context:\n",
    "\n",
    "* Week: 6\n",
    "* Lecture: 4 (Continued from previous lecture)\n",
    "* Focus: Derivation using Lagrangian, arriving at PCA algorithm, and worked-out example.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Recap of PCA Motivation\n",
    "\n",
    "We are given a dataset $x_1, x_2, \\dots, x_n \\in \\mathbb{R}^d$, and we aim to project it onto a lower-dimensional subspace $\\mathbb{R}^m$ such that the **reconstruction error** is minimized.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Optimization Problem ‚Äì A Simple Setup\n",
    "\n",
    "### üéØ Objective:\n",
    "\n",
    "Minimize:\n",
    "\n",
    "$$\n",
    "J^* = \\sum_{j=m+1}^{d} u_j^\\top C u_j\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "u_j^\\top u_j = 1\n",
    "$$\n",
    "\n",
    "This is a **constrained optimization problem**, and we solve it using **Lagrangian multipliers**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Solving with Lagrangian\n",
    "\n",
    "### üî£ Formulate Lagrangian:\n",
    "\n",
    "Let‚Äôs consider a simpler version:\n",
    "\n",
    "$$\n",
    "\\min \\, u^\\top C u \\quad \\text{subject to} \\quad u^\\top u = 1\n",
    "$$\n",
    "\n",
    "Define:\n",
    "\n",
    "$$\n",
    "L(u, \\lambda) = u^\\top C u - \\lambda(u^\\top u - 1)\n",
    "$$\n",
    "\n",
    "Differentiate $L$ w\\.r.t. $u$ and set to 0:\n",
    "\n",
    "$$\n",
    "\\nabla_u L = 2Cu - 2\\lambda u = 0 \\quad \\Rightarrow \\quad Cu = \\lambda u\n",
    "$$\n",
    "\n",
    "This is the **eigenvalue equation**. So, $u$ is an eigenvector and $\\lambda$ is the corresponding eigenvalue of matrix $C$.\n",
    "\n",
    "‚úÖ **Conclusion**:\n",
    "\n",
    "* The minimum of $u^\\top C u$ is achieved by choosing the **eigenvector corresponding to the smallest eigenvalue** of $C$.\n",
    "* Similarly, the largest projection (max variance) is in the direction of the **largest eigenvalue**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Properties of Covariance Matrix $C$\n",
    "\n",
    "Let:\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})(x_i - \\bar{x})^\\top\n",
    "$$\n",
    "\n",
    "* $C$ is **real and symmetric**\n",
    "* ‚áí All eigenvalues are **real**\n",
    "* ‚áí Eigenvectors are **orthonormal**\n",
    "* ‚áí There exists a **basis** of eigenvectors $u_1, u_2, ..., u_d$\n",
    "\n",
    "Let eigenvalues be ordered as:\n",
    "\n",
    "$$\n",
    "\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_d\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ PCA Algorithm Summary\n",
    "\n",
    "### ‚úçÔ∏è Given: Data $x_1, x_2, \\dots, x_n \\in \\mathbb{R}^d$\n",
    "\n",
    "### üìå Steps:\n",
    "\n",
    "1. **Compute Mean**:\n",
    "\n",
    "   $$\n",
    "   \\bar{x} = \\frac{1}{n} \\sum_{j=1}^{n} x_j\n",
    "   $$\n",
    "\n",
    "2. **Compute Covariance Matrix**:\n",
    "\n",
    "   $$\n",
    "   C = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})(x_i - \\bar{x})^\\top\n",
    "   $$\n",
    "\n",
    "3. **Compute Eigenvalues and Eigenvectors** of $C$:\n",
    "\n",
    "   $$\n",
    "   \\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_d\n",
    "   $$\n",
    "\n",
    "   With corresponding eigenvectors:\n",
    "\n",
    "   $$\n",
    "   u_1, u_2, ..., u_d\n",
    "   $$\n",
    "\n",
    "4. **Select Top-**$m$\\*\\* Eigenvectors\\*\\*:\n",
    "\n",
    "   * These define the lower-dimensional subspace\n",
    "   * $u_1, ..., u_m$ ‚Üê Top $m$ eigenvectors (largest eigenvalues)\n",
    "\n",
    "5. **Project Data**:\n",
    "\n",
    "   $$\n",
    "   \\tilde{x}_i = \\sum_{j=1}^{m} (x_i^\\top u_j) u_j + \\sum_{j=m+1}^{d} (\\bar{x}^\\top u_j) u_j\n",
    "   $$\n",
    "\n",
    "   * In centered data, $\\bar{x} = 0$, so:\n",
    "\n",
    "     $$\n",
    "     \\tilde{x}_i = \\sum_{j=1}^{m} (x_i^\\top u_j) u_j\n",
    "     $$\n",
    "\n",
    "6. **Minimization**:\n",
    "\n",
    "   * Reconstruction error is minimized by choosing $u_{m+1}, ..., u_d$ as the eigenvectors corresponding to the **smallest eigenvalues**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Example: 2D to 1D PCA\n",
    "\n",
    "### üßæ Data Points:\n",
    "\n",
    "$$\n",
    "x_1 = (-1, -1), \\quad x_2 = (0, 0), \\quad x_3 = (1, 1)\n",
    "$$\n",
    "\n",
    "### Step 1: Centering the Data\n",
    "\n",
    "* $\\bar{x} = (0, 0)$, so already centered\n",
    "\n",
    "### Step 2: Covariance Matrix\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{3} \\sum_{i=1}^{3} x_i x_i^\\top\n",
    "$$\n",
    "\n",
    "Compute each:\n",
    "\n",
    "$$\n",
    "x_1 x_1^\\top = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}, \\quad\n",
    "x_2 x_2^\\top = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}, \\quad\n",
    "x_3 x_3^\\top = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{3} \\left( \\begin{bmatrix} 2 & 2 \\\\ 2 & 2 \\end{bmatrix} \\right)\n",
    "= \\frac{2}{3} \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Step 3: Eigenvalues and Eigenvectors\n",
    "\n",
    "Characteristic equation:\n",
    "\n",
    "$$\n",
    "\\det(C - \\lambda I) = 0\n",
    "\\Rightarrow \\left| \\begin{bmatrix} \\frac{2}{3}-\\lambda & \\frac{2}{3} \\\\ \\frac{2}{3} & \\frac{2}{3}-\\lambda \\end{bmatrix} \\right| = 0\n",
    "\\Rightarrow (\\frac{2}{3}-\\lambda)^2 - \\left(\\frac{2}{3}\\right)^2 = 0\n",
    "\\Rightarrow \\lambda = \\frac{4}{3}, 0\n",
    "$$\n",
    "\n",
    "Eigenvectors:\n",
    "\n",
    "* $u_1 = \\frac{1}{\\sqrt{2}}(1, 1)^\\top$\n",
    "* $u_2 = \\frac{1}{\\sqrt{2}}(1, -1)^\\top$\n",
    "\n",
    "### Step 4: Projection\n",
    "\n",
    "We choose 1D projection, so use $u_1$:\n",
    "\n",
    "$$\n",
    "\\tilde{x}_i = (x_i^\\top u_1) u_1\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "* $x_1 = (-1, -1) \\Rightarrow \\tilde{x}_1 = -\\sqrt{2} \\cdot \\frac{1}{\\sqrt{2}} (1,1) = (-1, -1)$\n",
    "* $x_2 = (0, 0) \\Rightarrow \\tilde{x}_2 = (0, 0)$\n",
    "* $x_3 = (1, 1) \\Rightarrow \\tilde{x}_3 = (1, 1)$\n",
    "\n",
    "So the data was already on a 1D line. PCA **does nothing** in this case.\n",
    "\n",
    "### Step 5: Reconstruction Error\n",
    "\n",
    "Since projections equal original points:\n",
    "\n",
    "$$\n",
    "J^* = \\frac{1}{3} \\sum_{i=1}^3 \\|x_i - \\tilde{x}_i\\|^2 = 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Final Summary\n",
    "\n",
    "| Concept         | Explanation                                                            |\n",
    "| --------------- | ---------------------------------------------------------------------- |\n",
    "| **Goal**        | Reduce dimension while preserving structure (low reconstruction error) |\n",
    "| **Method**      | Use eigen-decomposition of the covariance matrix                       |\n",
    "| **Key Step**    | Choose top-m eigenvectors (max variance), project data                 |\n",
    "| **Mathematics** | Use Lagrangian to solve constrained optimization                       |\n",
    "| **Error**       | Reconstruction error minimized using smallest eigenvalues              |\n",
    "\n",
    "---\n",
    "\n",
    "## üîú What‚Äôs Next?\n",
    "\n",
    "In the next lecture, PCA will be interpreted from the **variance maximization** perspective, offering a dual view:\n",
    "\n",
    "* We've seen **minimizing reconstruction error**\n",
    "* Next: **maximizing variance of projected data**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
