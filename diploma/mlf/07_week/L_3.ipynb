{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1d047e",
   "metadata": {},
   "source": [
    "Here‚Äôs a **complete and in-depth note** on **PCA as Maximizing Variance**, covering all the topics from the lecture:\n",
    "\n",
    "---\n",
    "\n",
    "# üìå Principal Component Analysis (PCA) as Maximizing Variance\n",
    "\n",
    "**Machine Learning Foundations ‚Äî Lecture 5, Week 6**\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Recap: Two Perspectives on PCA\n",
    "\n",
    "Principal Component Analysis (PCA) is a fundamental technique in dimensionality reduction. There are **two equivalent perspectives**:\n",
    "\n",
    "1. **Minimizing Reconstruction Error**:\n",
    "\n",
    "   * Choose a lower-dimensional subspace that preserves the structure of the original data.\n",
    "   * Minimize the squared distance between original data and its projection onto the subspace.\n",
    "\n",
    "2. **Maximizing Projected Variance**:\n",
    "\n",
    "   * Find a subspace such that **variance of data after projection is maximized**.\n",
    "   * Variance is a measure of how spread out the data is ‚Äî more spread (variance) implies more information is retained.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Intuition Behind Maximizing Variance\n",
    "\n",
    "Let‚Äôs assume we have a dataset:\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{x_1, x_2, \\dots, x_n\\}, \\quad x_i \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "We want to **project** this data onto a 1D line (or m-dimensional subspace), such that the **variance of the projected data is maximized**.\n",
    "\n",
    "### üìê Projection Onto a Line\n",
    "\n",
    "Let $u \\in \\mathbb{R}^d$ be a **unit vector** (i.e., $u^T u = 1$) representing the direction onto which we project data.\n",
    "\n",
    "The projection of a data point $x_i$ onto $u$ is:\n",
    "\n",
    "$$\n",
    "\\text{Projection of } x_i \\text{ on } u = x_i^T u \\cdot u\n",
    "$$\n",
    "\n",
    "But for variance, we are interested in scalar projections:\n",
    "\n",
    "$$\n",
    "\\text{Projected scalar value} = x_i^T u\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Step-by-Step Derivation: Maximizing Variance\n",
    "\n",
    "---\n",
    "\n",
    "### 1Ô∏è‚É£ Define the Mean\n",
    "\n",
    "Let $\\bar{x}$ be the **mean** of the data:\n",
    "\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "We center the data by subtracting the mean (assumed centered unless stated otherwise).\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Projected Variance\n",
    "\n",
    "We define the **projected variance** as:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^{n} \\left( x_i^T u - \\bar{x}^T u \\right)^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\left( (x_i - \\bar{x})^T u \\right)^2\n",
    "$$\n",
    "\n",
    "This can be rewritten using matrix notation:\n",
    "\n",
    "$$\n",
    "= \\frac{1}{n} \\sum_{i=1}^{n} u^T (x_i - \\bar{x})(x_i - \\bar{x})^T u = u^T C u\n",
    "$$\n",
    "\n",
    "Where $C$ is the **covariance matrix**:\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})(x_i - \\bar{x})^T\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Objective: Maximize Projected Variance\n",
    "\n",
    "The goal is to **maximize the projected variance**:\n",
    "\n",
    "$$\n",
    "\\max_{u} u^T C u \\quad \\text{subject to} \\quad u^T u = 1\n",
    "$$\n",
    "\n",
    "This is a **constrained optimization** problem.\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ Solution Using Lagrangian\n",
    "\n",
    "We form the **Lagrangian**:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(u, \\lambda) = u^T C u - \\lambda (u^T u - 1)\n",
    "$$\n",
    "\n",
    "Taking the gradient and setting it to zero:\n",
    "\n",
    "$$\n",
    "\\nabla_u \\mathcal{L} = 2Cu - 2\\lambda u = 0 \\Rightarrow Cu = \\lambda u\n",
    "$$\n",
    "\n",
    "Thus, **u is an eigenvector of C**, and the **projected variance** is:\n",
    "\n",
    "$$\n",
    "u^T C u = \\lambda\n",
    "$$\n",
    "\n",
    "Hence, to **maximize** variance, choose **the eigenvector corresponding to the largest eigenvalue**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ Alternative: Calculus Without Lagrangian\n",
    "\n",
    "To avoid the Lagrangian, consider maximizing the **Rayleigh quotient**:\n",
    "\n",
    "$$\n",
    "R(u) = \\frac{u^T C u}{u^T u}\n",
    "$$\n",
    "\n",
    "We differentiate this ratio (using quotient rule) and again reach the conclusion:\n",
    "\n",
    "$$\n",
    "Cu = \\lambda u \\quad \\text{where} \\quad \\lambda = \\frac{u^T C u}{u^T u}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üìè Generalization to m-Dimensional Subspace\n",
    "\n",
    "If we want to reduce to **m dimensions**, not just one, we want to find **m orthonormal vectors $u_1, u_2, ..., u_m$** that:\n",
    "\n",
    "* Maximize the **total projected variance**:\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^m u_j^T C u_j\n",
    "$$\n",
    "\n",
    "* Subject to:\n",
    "\n",
    "$$\n",
    "u_i^T u_j = \\delta_{ij} \\quad (\\text{orthonormality})\n",
    "$$\n",
    "\n",
    "The solution: **top m eigenvectors** of the covariance matrix $C$, corresponding to the **top m eigenvalues**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Terminology\n",
    "\n",
    "* **Principal Directions** $\\rightarrow$ Eigenvectors $u_1, u_2, ..., u_m$ of $C$\n",
    "* **Principal Components** $\\rightarrow$ Projected values $x_i^T u_j$\n",
    "* The **projected data** lies in an m-dimensional subspace of $\\mathbb{R}^d$\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Example: 2D Dataset\n",
    "\n",
    "Given data points:\n",
    "\n",
    "$$\n",
    "x_1 = [-1, -1],\\quad x_2 = [0, 0],\\quad x_3 = [1, 1]\n",
    "$$\n",
    "\n",
    "### Step 1: Compute the mean\n",
    "\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{3}(x_1 + x_2 + x_3) = [0, 0]\n",
    "$$\n",
    "\n",
    "### Step 2: Compute Covariance Matrix\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{3} \\sum_{i=1}^{3} x_i x_i^T =\n",
    "\\frac{1}{3} \\left( \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} + \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix} + \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\right)\n",
    "= \\begin{bmatrix} \\frac{2}{3} & \\frac{2}{3} \\\\ \\frac{2}{3} & \\frac{2}{3} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Step 3: Eigenvalues and Eigenvectors\n",
    "\n",
    "* Eigenvalues: $\\lambda_1 = \\frac{4}{3}, \\lambda_2 = 0$\n",
    "* Corresponding eigenvectors:\n",
    "  $u_1 = \\frac{1}{\\sqrt{2}}[1, 1]^T$,\n",
    "  $u_2 = \\frac{1}{\\sqrt{2}}[1, -1]^T$\n",
    "\n",
    "### Step 4: Projection onto $u_1$\n",
    "\n",
    "Compute projected values (principal components):\n",
    "\n",
    "$$\n",
    "x_1^T u_1 = [-1, -1] \\cdot \\frac{1}{\\sqrt{2}}[1, 1] = -\\sqrt{2} \\\\\n",
    "x_2^T u_1 = [0, 0] \\cdot \\frac{1}{\\sqrt{2}}[1, 1] = 0 \\\\\n",
    "x_3^T u_1 = [1, 1] \\cdot \\frac{1}{\\sqrt{2}}[1, 1] = \\sqrt{2}\n",
    "$$\n",
    "\n",
    "### Step 5: Compute Projected Variance\n",
    "\n",
    "$$\n",
    "\\text{Projected Variance} = \\frac{1}{3} \\left[ (-\\sqrt{2})^2 + 0^2 + (\\sqrt{2})^2 \\right] = \\frac{1}{3}(2 + 0 + 2) = \\frac{4}{3}\n",
    "$$\n",
    "\n",
    "This is equal to the **largest eigenvalue of C**, which validates our result.\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Summary: PCA as Maximizing Variance\n",
    "\n",
    "| Concept                  | Summary                                                                                 |\n",
    "| ------------------------ | --------------------------------------------------------------------------------------- |\n",
    "| **Goal**                 | Find the directions (principal directions) in which the data has maximum variance       |\n",
    "| **How**                  | Maximize $u^T C u$ s.t. $u^T u = 1$                                                     |\n",
    "| **Solution**             | Take eigenvectors of the covariance matrix $C$ corresponding to the largest eigenvalues |\n",
    "| **Principal Directions** | Eigenvectors $u_1, u_2, ..., u_m$                                                       |\n",
    "| **Principal Components** | Projections $x_i^T u_j$                                                                 |\n",
    "| **Projected Variance**   | Equal to eigenvalues $\\lambda_1, ..., \\lambda_m$                                        |\n",
    "| **Algorithm Output**     | Subspace of dimension m, variance-maximizing projection                                 |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Learning Outcomes\n",
    "\n",
    "* ‚úÖ Understood PCA as a method to maximize projected variance.\n",
    "* ‚úÖ Learned that the solution involves computing the **eigenvectors of the covariance matrix**.\n",
    "* ‚úÖ Understood that PCA outputs the **principal components** by projecting data onto **principal directions**.\n",
    "* ‚úÖ Verified the result through a **worked-out example**.\n",
    "* ‚úÖ Understood that this method is equivalent to **minimizing reconstruction error**, hence the same algorithm serves **two objectives**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
