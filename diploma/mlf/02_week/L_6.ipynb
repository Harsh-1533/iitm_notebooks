{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16948c27",
   "metadata": {},
   "source": [
    "# Gradients and Linear Approximations\n",
    "\n",
    "We consider a scalar function:\n",
    "\n",
    "$$\n",
    "f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Let $v = (v_1, v_2) \\in \\mathbb{R}^2$ be a point where we evaluate the gradient, and $y = (y_1, y_2) \\in \\mathbb{R}^2$ be a nearby point.\n",
    "\n",
    "### ðŸ”¹ Step-by-step Linear Approximation\n",
    "\n",
    "#### Step 1: Approximate along $x_1$-direction (keeping $x_2 = v_2$)\n",
    "\n",
    "$$\n",
    "f(y_1, v_2) \\approx f(v_1, v_2) + \\frac{\\partial f}{\\partial x_1}(v) \\cdot (y_1 - v_1)\n",
    "$$\n",
    "\n",
    "#### Step 2: Rearranged Form\n",
    "\n",
    "$$\n",
    "f(y_1, v_2) - f(v_1, v_2) \\approx \\frac{\\partial f}{\\partial x_1}(v) \\cdot (y_1 - v_1)\n",
    "$$\n",
    "\n",
    "#### Step 3: Approximate along $x_2$-direction (keeping $x_1 = v_1$)\n",
    "\n",
    "$$\n",
    "f(v_1, y_2) - f(v_1, v_2) \\approx \\frac{\\partial f}{\\partial x_2}(v) \\cdot (y_2 - v_2)\n",
    "$$\n",
    "\n",
    "#### Step 4: Combine both directions\n",
    "\n",
    "$$\n",
    "f(y_1, y_2) - f(v_1, v_2) \\approx \\frac{\\partial f}{\\partial x_1}(v) \\cdot (y_1 - v_1) + \\frac{\\partial f}{\\partial x_2}(v) \\cdot (y_2 - v_2)\n",
    "$$\n",
    "\n",
    "#### âœ… Final Compact Form (Using Gradient Vector)\n",
    "\n",
    "$$\n",
    "f(y_1, y_2) \\approx f(v_1, v_2) + \\nabla f(v)^T \\cdot (y - v)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\nabla f(v) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1}(v) \\\\ \\frac{\\partial f}{\\partial x_2}(v) \\end{bmatrix}$\n",
    "* $y - v = \\begin{bmatrix} y_1 - v_1 \\\\ y_2 - v_2 \\end{bmatrix}$\n",
    "\n",
    "So the dot product:\n",
    "\n",
    "$$\n",
    "\\nabla f(v)^T (y - v) = \\frac{\\partial f}{\\partial x_1}(v)(y_1 - v_1) + \\frac{\\partial f}{\\partial x_2}(v)(y_2 - v_2)\n",
    "$$\n",
    "\n",
    "This is the **first-order Taylor approximation** of a multivariable function near point $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d31de4",
   "metadata": {},
   "source": [
    "Let:\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = x_1^2 + x_2^2, \\quad \\nabla f(x) = \\begin{bmatrix} 2x_1 \\\\ 2x_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### i) Approximate \\( f \\) around \\( (6, 2) \\):\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "f(v) = 40, \\quad \\nabla f(v) = \\begin{bmatrix} 12 \\\\ 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "f(x) \\approx 40 + \\begin{bmatrix} 12 & 4 \\end{bmatrix} \\begin{bmatrix} x_1 - 6 \\\\ x_2 - 2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "= 40 + 12(x_1 - 6) + 4(x_2 - 2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 40 + 12x_1 + 4x_2 - 72 - 8\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 12x_1 + 4x_2 - 40\n",
    "$$\n",
    "\n",
    "Where \\( (x_1, x_2) \\in \\mathbb{R}^2 \\)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254da1cd",
   "metadata": {},
   "source": [
    "## Gradients and Tangent Planes\n",
    "- The graph of $ L_v[f] $ is a **plane** that is **tangent** to the graph of \\( f \\) at the point \\( (v, f(v)) \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e92250c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16ecf204",
   "metadata": {},
   "source": [
    "## Gradients and Contours\n",
    "\n",
    "Let:\n",
    "\n",
    "$$\n",
    "v = \\begin{bmatrix} -6 \\\\ 2 \\end{bmatrix}, \\quad \\nabla f(v) = \\begin{bmatrix} -12 \\\\ 4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We know that the gradient at \\( v \\) is perpendicular to the level set (contour line) of \\( f \\) at that point.\n",
    "\n",
    "$$\n",
    "\\nabla f(v) \\perp \\left\\{ x \\in \\mathbb{R}^d : f(x) = f(v) \\right\\}\n",
    "$$\n",
    "\n",
    "This also implies:\n",
    "\n",
    "$$\n",
    "\\nabla f(v) \\perp \\left\\{ x \\in \\mathbb{R}^d : L_v[f](x) = f(v) \\right\\}\n",
    "$$\n",
    "\n",
    "Which leads to:\n",
    "\n",
    "$$\n",
    "\\left\\{ x \\in \\mathbb{R}^d : f(v) + \\nabla f(v)^T (x - v) = f(v) \\right\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left\\{ x \\in \\mathbb{R}^d : \\nabla f(v)^T (x - v) = 0 \\right\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\left\\{ x \\in \\mathbb{R}^d : w^T x = b \\right\\}\n",
    "$$\n",
    "\n",
    "This represents a **hyperplane** that is tangent to the level set at \\( v \\), and the gradient is orthogonal (normal) to this hyperplane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e032767f",
   "metadata": {},
   "source": [
    "## Directional Derivative\n",
    "\n",
    "The directional derivative of $ f $ at the point $ v $, along direction $ u $, is defined as:\n",
    "\n",
    "$$\n",
    "D_u[f](v) = \\lim_{\\alpha \\to 0} \\frac{f(v + \\alpha u) - f(v)}{\\alpha}\n",
    "$$\n",
    "\n",
    "Using the first-order Taylor expansion:\n",
    "\n",
    "$$\n",
    "= \\lim_{\\alpha \\to 0} \\frac{f(v) + \\nabla f(v)^T \\alpha u - f(v)}{\\alpha}\n",
    "$$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$$\n",
    "= \\nabla f(v)^T u\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40390899",
   "metadata": {},
   "source": [
    "## Cauchyâ€“Schwarz Inequality\n",
    "\n",
    "Let \n",
    "\n",
    "$$\n",
    "a = (a_1, a_2, \\dots, a_d), \\quad b = (b_1, b_2, \\dots, b_d)\n",
    "$$\n",
    "\n",
    "The norm of a vector \\( a \\) is:\n",
    "\n",
    "$$\n",
    "\\|a\\| = \\sqrt{a_1^2 + \\cdots + a_d^2}\n",
    "$$\n",
    "\n",
    "The Cauchyâ€“Schwarz inequality states:\n",
    "\n",
    "$$\n",
    "- \\|a\\| \\cdot \\|b\\| \\leq a^T b \\leq \\|a\\| \\cdot \\|b\\|\n",
    "$$\n",
    "\n",
    "- If $ a = \\alpha b $ and $ \\alpha < 0 $, then $ a^T b \\leq 0 $\n",
    "- If $ a = \\alpha b $ and $ \\alpha > 0 $, then $ a^T b \\geq 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eec730",
   "metadata": {},
   "source": [
    "## Direction of Steepest Ascent\n",
    "\n",
    "Let $ f $ be a differentiable function.  \n",
    "We aim to find a direction $ u $ that maximizes the rate of change of $ f $ as you move from $ v $ along $ u $.\n",
    "\n",
    "Maximize:\n",
    "\n",
    "$$\n",
    "D_u[f](v)\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$\n",
    "u \\in \\mathbb{R}^d, \\quad \\|u\\| = 1\n",
    "$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "D_u[f](v) = \\nabla f(v)^T u\n",
    "$$\n",
    "\n",
    "To maximize $ \\nabla f(v)^T u $ under the constraint $ \\|u\\| = 1 $,  \n",
    "choose:\n",
    "\n",
    "$$\n",
    "u = \\alpha \\nabla f(v)\n",
    "$$\n",
    "\n",
    "for some scalar $ \\alpha $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ef777e",
   "metadata": {},
   "source": [
    "### Descent Directions\n",
    "\n",
    "Let  \n",
    "$f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$  \n",
    "$v \\in \\mathbb{R}^d$\n",
    "\n",
    "What are the **valid directions**, such that $f$ decreases?\n",
    "\n",
    "For what values of $u$:  \n",
    "$D_u[f](v) < 0$  \n",
    "$\\Downarrow$  \n",
    "$\\nabla f(v)^T u < 0$\n",
    "\n",
    "**Descent directions**:  \n",
    "$\\left\\{ u \\in \\mathbb{R}^d : \\nabla f(v)^T u < 0 \\right\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db637b61",
   "metadata": {},
   "source": [
    "### Higher Order Approximations\n",
    "\n",
    "Let  \n",
    "$f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$\n",
    "\n",
    "---\n",
    "\n",
    "**First-order approximation:**\n",
    "\n",
    "$$\n",
    "f(x) \\approx f(v) + \\nabla f(v)^T (x - v)\n",
    "\\quad \\text{(Valid around } x = v \\text{)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Second-order approximation:**\n",
    "\n",
    "$$\n",
    "f(x) \\approx f(v) + \\nabla f(v)^T (x - v) + \\frac{1}{2} (x - v)^T \\nabla^2 f(v) (x - v)\n",
    "$$\n",
    "\n",
    "Here,  \n",
    "$\\nabla^2 f(v)$ is the **Hessian**, a $d \\times d$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ead801",
   "metadata": {},
   "source": [
    "### Maxima, Minima, and Saddle Points\n",
    "\n",
    "If $f(x)$ is minimised at $v$,  \n",
    "then  \n",
    "$$\n",
    "\\nabla f(v) = 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "The set of points  \n",
    "$$\n",
    "\\left\\{ v : \\nabla f(v) = 0 \\right\\}\n",
    "$$  \n",
    "are called **critical points**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
