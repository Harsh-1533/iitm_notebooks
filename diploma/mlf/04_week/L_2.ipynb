{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "821167ba",
   "metadata": {},
   "source": [
    "Understanding **eigenvalues** and **eigenvectors** is crucial in many areas of **machine learning**, especially in algorithms involving matrices, such as **PCA**, **linear transformations**, **optimization**, and more. Let‚Äôs break it down clearly.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∑ What Are Eigenvalues and Eigenvectors?\n",
    "\n",
    "### ‚úÖ Definition\n",
    "\n",
    "Given a **square matrix** $A \\in \\mathbb{R}^{n \\times n}$, a **non-zero vector** $\\mathbf{v}$ is called an **eigenvector** of $A$ if:\n",
    "\n",
    "$$\n",
    "A \\mathbf{v} = \\lambda \\mathbf{v}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\lambda$ is a **scalar** called the **eigenvalue**.\n",
    "* $\\mathbf{v}$ is the **eigenvector** corresponding to $\\lambda$.\n",
    "\n",
    "This means:\n",
    "\n",
    "* Applying the transformation $A$ to the vector $\\mathbf{v}$ **does not change its direction**, only **scales** it by $\\lambda$.\n",
    "\n",
    "---\n",
    "\n",
    "## üî∂ Intuition with Geometry\n",
    "\n",
    "* Imagine stretching or rotating space using a matrix.\n",
    "* **Eigenvectors** are directions that stay the **same** after the transformation (no rotation).\n",
    "* **Eigenvalues** tell **how much** that direction is stretched or shrunk.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå How Do We Find Them?\n",
    "\n",
    "From the equation:\n",
    "\n",
    "$$\n",
    "A \\mathbf{v} = \\lambda \\mathbf{v} \\Rightarrow (A - \\lambda I)\\mathbf{v} = 0\n",
    "$$\n",
    "\n",
    "We solve:\n",
    "\n",
    "$$\n",
    "\\det(A - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "This gives us the **characteristic polynomial** ‚Üí solving it gives **eigenvalues** $\\lambda$. Then, substitute back to get corresponding **eigenvectors**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Why Are Eigenvalues/Eigenvectors Important in Machine Learning?\n",
    "\n",
    "### 1. **Principal Component Analysis (PCA)**\n",
    "\n",
    "* PCA uses **eigenvectors** of the **covariance matrix** to find directions (principal components) where data has **maximum variance**.\n",
    "* The eigenvectors = principal directions\n",
    "* The eigenvalues = variance explained along each direction\n",
    "\n",
    "### 2. **Data Compression & Dimensionality Reduction**\n",
    "\n",
    "* PCA helps reduce features by **keeping only top-k eigenvectors** with highest eigenvalues.\n",
    "* This keeps **most of the information** while removing noise.\n",
    "\n",
    "### 3. **Spectral Clustering**\n",
    "\n",
    "* Uses **eigenvectors of a graph Laplacian matrix** to cluster data in a transformed space.\n",
    "\n",
    "### 4. **Linear Transformations**\n",
    "\n",
    "* Eigenvalues give insight into how a linear transformation **scales** different directions.\n",
    "* Helps understand **stability** and **convergence** in optimization algorithms like gradient descent.\n",
    "\n",
    "### 5. **Markov Chains**\n",
    "\n",
    "* Long-term behavior is governed by **dominant eigenvalue/eigenvector** (stationary distribution).\n",
    "\n",
    "### 6. **Covariance Matrix**\n",
    "\n",
    "* In statistics and ML, covariance matrices are often analyzed via their **eigenvalues** (spread/variability) and **eigenvectors** (directions of variance).\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Example in PCA (Simplified)\n",
    "\n",
    "Given dataset $X \\in \\mathbb{R}^{n \\times d}$:\n",
    "\n",
    "1. Center the data: $X_{\\text{centered}} = X - \\mu$\n",
    "2. Compute covariance: $C = \\frac{1}{n} X_{\\text{centered}}^T X_{\\text{centered}}$\n",
    "3. Compute **eigenvalues** and **eigenvectors** of $C$\n",
    "4. Sort by eigenvalue magnitude\n",
    "5. Project data using top-k eigenvectors (principal components)\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Simple 2D Example\n",
    "\n",
    "Let‚Äôs say:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "2 & 0 \\\\\n",
    "0 & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* Eigenvectors: $[1,0]^T$ and $[0,1]^T$\n",
    "* Eigenvalues: $\\lambda_1 = 2, \\lambda_2 = 3$\n",
    "\n",
    "This means:\n",
    "\n",
    "* Any vector on x-axis is stretched by 2\n",
    "* Any vector on y-axis is stretched by 3\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary Table\n",
    "\n",
    "| Concept          | Role in ML                                 |\n",
    "| ---------------- | ------------------------------------------ |\n",
    "| **Eigenvector**  | Direction of transformation                |\n",
    "| **Eigenvalue**   | Scale of transformation                    |\n",
    "| **PCA**          | Top eigenvectors of covariance matrix      |\n",
    "| **Clustering**   | Spectral methods use graph eigenvectors    |\n",
    "| **Optimization** | Convergence depends on Hessian eigenvalues |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cf2974",
   "metadata": {},
   "source": [
    "Let‚Äôs go through an **in-depth example** of **eigenvalues and eigenvectors** with full calculation and then show how this connects to **machine learning**, especially **PCA (Principal Component Analysis)**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Example: Small 2√ó2 Matrix\n",
    "\n",
    "Let‚Äôs take a simple matrix:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "2 & 1 \\\\\n",
    "1 & 2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We want to find **eigenvalues** and **eigenvectors**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Step 1: Solve the Characteristic Equation\n",
    "\n",
    "We solve:\n",
    "\n",
    "$$\n",
    "\\det(A - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\det \\left(\n",
    "\\begin{bmatrix}\n",
    "2 & 1 \\\\\n",
    "1 & 2\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\lambda\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "= 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow \\det \\left(\n",
    "\\begin{bmatrix}\n",
    "2-\\lambda & 1 \\\\\n",
    "1 & 2-\\lambda\n",
    "\\end{bmatrix}\n",
    "\\right) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "(2 - \\lambda)^2 - 1 = 0\n",
    "\\Rightarrow \\lambda^2 - 4\\lambda + 3 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow (\\lambda - 1)(\\lambda - 3) = 0\n",
    "$$\n",
    "\n",
    "So, **eigenvalues** are:\n",
    "\n",
    "$$\n",
    "\\lambda_1 = 1, \\quad \\lambda_2 = 3\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Step 2: Find Eigenvectors\n",
    "\n",
    "#### For $\\lambda = 1$:\n",
    "\n",
    "Solve:\n",
    "\n",
    "$$\n",
    "(A - I)v = 0\n",
    "\\Rightarrow\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{bmatrix}\n",
    "= 0\n",
    "$$\n",
    "\n",
    "Gives:\n",
    "\n",
    "$$\n",
    "x + y = 0 \\Rightarrow y = -x\n",
    "$$\n",
    "\n",
    "So one eigenvector:\n",
    "\n",
    "$$\n",
    "v_1 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### For $\\lambda = 3$:\n",
    "\n",
    "$$\n",
    "(A - 3I)v = 0\n",
    "\\Rightarrow\n",
    "\\begin{bmatrix}\n",
    "-1 & 1 \\\\\n",
    "1 & -1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{bmatrix}\n",
    "= 0\n",
    "$$\n",
    "\n",
    "Gives:\n",
    "\n",
    "$$\n",
    "-x + y = 0 \\Rightarrow y = x\n",
    "$$\n",
    "\n",
    "So another eigenvector:\n",
    "\n",
    "$$\n",
    "v_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "| Eigenvalue $\\lambda$ | Eigenvector $v$                         |\n",
    "| -------------------- | --------------------------------------- |\n",
    "| 1                    | $\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$ |\n",
    "| 3                    | $\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$  |\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Machine Learning Connection ‚Äî PCA\n",
    "\n",
    "Suppose we have a **2D dataset**:\n",
    "\n",
    "```\n",
    "X = [[2, 0],\n",
    "     [0, 2],\n",
    "     [1, 1],\n",
    "     [3, 1]]\n",
    "```\n",
    "\n",
    "### PCA Steps:\n",
    "\n",
    "1. **Center the data**\n",
    "2. **Compute covariance matrix** of $X$\n",
    "3. **Find eigenvectors and eigenvalues** of covariance matrix\n",
    "4. **Project data onto top eigenvector(s)**\n",
    "\n",
    "Eigenvectors are **directions of maximum variance**, and eigenvalues show **how much variance** lies in that direction.\n",
    "\n",
    "In our example matrix $A$, the eigenvector\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "represents the direction of **maximal variance** in data ‚Üí PCA would **project** onto this line if we wanted 1D compression.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Visual Intuition\n",
    "\n",
    "* Original data is scattered in 2D space.\n",
    "* Eigenvectors point along directions of major spread.\n",
    "* PCA uses those to rotate axes ‚Üí so you can drop dimensions (compression) without losing much information.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
