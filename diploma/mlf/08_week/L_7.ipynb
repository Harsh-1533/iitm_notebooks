{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb015a9",
   "metadata": {},
   "source": [
    "Here is an **in-depth note on Lecture L8.7: Gradient Descent for Multivariate Functions** from the *Foundations of Machine Learning Theory* course. This note is structured for deep understanding, step-by-step derivations, and full coverage of all concepts mentioned in the transcript.\n",
    "\n",
    "---\n",
    "\n",
    "# üìò L8.7: **Gradient Descent for Multivariate Functions**\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Goal of the Lecture**\n",
    "\n",
    "To generalize the idea of **gradient descent** (which we studied for single-variable functions) to **functions of multiple variables** (i.e., multivariate functions).\n",
    "\n",
    "---\n",
    "\n",
    "## üìå **1. From Single Variable to Multivariate**\n",
    "\n",
    "* In single-variable optimization, we use the **derivative** to guide descent:\n",
    "\n",
    "  $$\n",
    "  x_{t+1} = x_t - \\eta f'(x_t)\n",
    "  $$\n",
    "\n",
    "* In multivariate optimization, the function is now:\n",
    "\n",
    "  $$\n",
    "  f: \\mathbb{R}^n \\rightarrow \\mathbb{R}\n",
    "  $$\n",
    "\n",
    "  Example:\n",
    "\n",
    "  $$\n",
    "  f(x_1, x_2) = x_1^2 + 4x_2 + 8x_2^2\n",
    "  $$\n",
    "\n",
    "* Question: **What is the analogue of the derivative in higher dimensions?**\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **2. Introducing the Gradient**\n",
    "\n",
    "### ‚úÖ Definition:\n",
    "\n",
    "> The **gradient** of a multivariate function is a **vector of partial derivatives** with respect to each input variable.\n",
    "\n",
    "For a function $f(x_1, x_2, \\dots, x_n)$, the gradient is:\n",
    "\n",
    "$$\n",
    "\\nabla f(x) = \\left[ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right]^T\n",
    "$$\n",
    "\n",
    "This vector points in the direction of **steepest increase** of the function.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç **3. Understanding Partial Derivatives**\n",
    "\n",
    "To compute partial derivatives:\n",
    "\n",
    "* Fix all variables except one.\n",
    "* Differentiate the function with respect to that variable.\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = x_1^2 + 4x_2 + 8x_2^2\n",
    "$$\n",
    "\n",
    "Compute:\n",
    "\n",
    "* $\\frac{\\partial f}{\\partial x_1} = 2x_1$\n",
    "* $\\frac{\\partial f}{\\partial x_2} = 4 + 16x_2$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\nabla f(x_1, x_2) = \\begin{bmatrix}\n",
    "2x_1 \\\\\n",
    "4 + 16x_2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ **4. Example: Compute Gradient at a Point**\n",
    "\n",
    "Let:\n",
    "\n",
    "$$\n",
    "f(x_1, x_2) = x_1^2 + 4x_2 + 8x_2^2\n",
    "$$\n",
    "\n",
    "Evaluate at $(x_1, x_2) = (1, 3)$:\n",
    "\n",
    "$$\n",
    "\\nabla f(1, 3) = \\begin{bmatrix}\n",
    "2(1) \\\\\n",
    "4 + 16(3)\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "2 \\\\\n",
    "52\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ **5. Gradient as a Directional Vector**\n",
    "\n",
    "* The gradient vector indicates a **direction** and **magnitude**.\n",
    "* In **gradient descent**, we move **opposite** to the gradient to **minimize** the function.\n",
    "\n",
    "> Direction of descent = $-\\nabla f(x)$\n",
    "\n",
    "---\n",
    "\n",
    "## üêÑ **6. Cow and Grass Example: Understanding Gradient Geometrically**\n",
    "\n",
    "### Scenario:\n",
    "\n",
    "* Grass is located at point $(40, 40)$.\n",
    "* Cow is at position $(x_1, x_2)$.\n",
    "* Distance function $D$ from cow to grass is:\n",
    "\n",
    "  $$\n",
    "  D(x_1, x_2) = (x_1 - 40)^2 + (x_2 - 40)^2\n",
    "  $$\n",
    "\n",
    "### Gradient of Distance:\n",
    "\n",
    "$$\n",
    "\\nabla D(x_1, x_2) = \\begin{bmatrix}\n",
    "2(x_1 - 40) \\\\\n",
    "2(x_2 - 40)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### At point $(5, 2)$:\n",
    "\n",
    "$$\n",
    "\\nabla D(5, 2) = \\begin{bmatrix}\n",
    "2(5 - 40) \\\\\n",
    "2(2 - 40)\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "-70 \\\\\n",
    "-76\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Negative gradient direction (for descent):**\n",
    "\n",
    "$$\n",
    "-\\nabla D(5, 2) = \\begin{bmatrix}\n",
    "70 \\\\\n",
    "76\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "‚û°Ô∏è This tells us to move toward the grass at (40, 40). The direction of the vector leads us closer to the minimum of the function (i.e., the grass).\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ **7. Another Point Example: (30, 50)**\n",
    "\n",
    "$$\n",
    "\\nabla D(30, 50) = \\begin{bmatrix}\n",
    "2(30 - 40) \\\\\n",
    "2(50 - 40)\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "-20 \\\\\n",
    "20\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "-\\nabla D(30, 50) = \\begin{bmatrix}\n",
    "20 \\\\\n",
    "-20\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* $+x$ direction\n",
    "* $-y$ direction\n",
    "\n",
    "‚û°Ô∏è This moves the cow closer to (40, 40), confirming again that **negative gradient direction leads to the minimum**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è **8. Gradient Descent Update Rule (Multivariate Case)**\n",
    "\n",
    "We now generalize the update rule:\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t - \\eta \\nabla f(x_t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $x_t$ is a **vector** of current parameters.\n",
    "* $\\eta$ is the **step size** (learning rate).\n",
    "* $\\nabla f(x_t)$ is the **gradient vector** at time $t$.\n",
    "\n",
    "This is called **gradient descent** for multivariate functions.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **9. Summary and Key Concepts**\n",
    "\n",
    "| Concept                     | Description                                                                                                     |\n",
    "| --------------------------- | --------------------------------------------------------------------------------------------------------------- |\n",
    "| **Gradient**                | Vector of partial derivatives. Points in direction of maximum increase of function.                             |\n",
    "| **Negative Gradient**       | Used in descent to find minimum.                                                                                |\n",
    "| **Gradient Descent Update** | $x_{t+1} = x_t - \\eta \\nabla f(x_t)$                                                                            |\n",
    "| **Partial Derivative**      | Derivative w\\.r.t. one variable, keeping others constant.                                                       |\n",
    "| **Direction of Descent**    | Given by $-\\nabla f(x)$, moves function value down.                                                             |\n",
    "| **Local Minimum**           | Point where function is minimal in a neighborhood (but not necessarily globally minimal).                       |\n",
    "| **Vector Interpretation**   | Gradient vectors indicate both direction and magnitude. Step sizes determine how far to move in that direction. |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Learning Outcome**\n",
    "\n",
    "By the end of this lecture, you should understand:\n",
    "\n",
    "* How to extend derivative-based optimization (1D) to higher dimensions.\n",
    "* The meaning and computation of the **gradient**.\n",
    "* How the gradient guides **gradient descent** in multivariate optimization.\n",
    "* Why moving in the **negative gradient direction** helps in minimizing the function.\n",
    "* How to implement **gradient descent** with vector notation for any differentiable multivariate function."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
