{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6547940c",
   "metadata": {},
   "source": [
    "Here are **in-depth notes** for **L8.6: Gradient Descent and Taylor Series** from the *Foundations of Machine Learning Theory* course. These notes cover each and every concept discussed in the lecture:\n",
    "\n",
    "---\n",
    "\n",
    "## 📘 L8.6: Gradient Descent and Taylor Series — In-Depth Notes\n",
    "\n",
    "### 🧠 Objective:\n",
    "\n",
    "To understand **why the gradient descent algorithm moves in the negative gradient direction** by using **Taylor Series** as a mathematical tool to justify the choice.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 📌 Recap: Gradient Descent Algorithm for Unconstrained Optimization\n",
    "\n",
    "* **Goal**: Minimize a real-valued function $f(x)$ without constraints.\n",
    "* **Update rule**:\n",
    "\n",
    "  $$\n",
    "  x_{t+1} = x_t - \\eta_t \\cdot f'(x_t)\n",
    "  $$\n",
    "\n",
    "  * $\\eta_t$: step size (learning rate)\n",
    "  * $f'(x_t)$: derivative (gradient) at $x_t$\n",
    "* Moves **iteratively** in the direction of **negative gradient**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ❓ Why Move in the Negative Gradient Direction?\n",
    "\n",
    "We ask:\n",
    "\n",
    "> What is special about $-f'(x)$?\n",
    "> Why does moving in this direction reduce the function value?\n",
    "\n",
    "To answer this, we use **Taylor Series Expansion**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 🔬 Taylor Series Expansion\n",
    "\n",
    "For a function $f$, we expand its value at $x + \\eta d$ using Taylor series:\n",
    "\n",
    "$$\n",
    "f(x + \\eta d) = f(x) + \\eta d \\cdot f'(x) + \\frac{(\\eta d)^2}{2} \\cdot f''(x) + \\cdots\n",
    "$$\n",
    "\n",
    "### Interpretation:\n",
    "\n",
    "* Let $x$ be the current point.\n",
    "* $d$ is an arbitrary direction.\n",
    "* $\\eta$ is a small step size.\n",
    "* This expansion shows the function’s value at a nearby point $x + \\eta d$ in terms of:\n",
    "\n",
    "  * Function value at $x$\n",
    "  * Derivatives at $x$\n",
    "\n",
    "📌 **Insight**: The function’s behavior at a nearby point is determined entirely by its **local behavior** (value and derivatives) at the current point.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 🌍 Local Information Gives Global Approximation\n",
    "\n",
    "### Key Idea:\n",
    "\n",
    "> \"Knowing the function's value and all its derivatives at a point $x$ lets you approximate the function value at any other point $x + \\eta d$.\"\n",
    "\n",
    "This is powerful in optimization:\n",
    "\n",
    "* You can predict how the function behaves in any direction using only **local** information.\n",
    "* Crucial when selecting a direction that minimizes $f(x)$.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. ✅ Application in Optimization: Choosing a Descent Direction\n",
    "\n",
    "Suppose we are at point $x$, and we consider moving in a direction $d$ with a small step size $\\eta$.\n",
    "\n",
    "### Use Taylor Approximation (First-order):\n",
    "\n",
    "$$\n",
    "f(x + \\eta d) \\approx f(x) + \\eta d \\cdot f'(x)\n",
    "$$\n",
    "\n",
    "(neglect higher-order terms for small $\\eta$)\n",
    "\n",
    "Now define the **change in function value**:\n",
    "\n",
    "$$\n",
    "f(x + \\eta d) - f(x) \\approx \\eta d \\cdot f'(x)\n",
    "$$\n",
    "\n",
    "### Goal in Optimization:\n",
    "\n",
    "We want the function value to **decrease**:\n",
    "\n",
    "$$\n",
    "f(x + \\eta d) - f(x) < 0\n",
    "\\Rightarrow \\eta d \\cdot f'(x) < 0\n",
    "\\Rightarrow d \\cdot f'(x) < 0\n",
    "$$\n",
    "\n",
    "(Note: $\\eta > 0$, so it doesn’t affect sign.)\n",
    "\n",
    "---\n",
    "\n",
    "## 6. 🎯 How to Choose the Direction $d$?\n",
    "\n",
    "We need to find a direction $d$ such that:\n",
    "\n",
    "$$\n",
    "d \\cdot f'(x) < 0\n",
    "$$\n",
    "\n",
    "This guarantees the function will decrease when moving from $x$ to $x + \\eta d$.\n",
    "\n",
    "### Best Choice:\n",
    "\n",
    "Let $d = -f'(x)$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "d \\cdot f'(x) = (-f'(x)) \\cdot f'(x) = -[f'(x)]^2 < 0 \\quad (\\text{unless } f'(x) = 0)\n",
    "$$\n",
    "\n",
    "So, the **negative gradient** direction always guarantees **decrease in function value**, unless you're already at a stationary point.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. 🧠 Intuition Summary\n",
    "\n",
    "* The **Taylor series** tells us how the function behaves near a point.\n",
    "* If $\\eta$ is small, we can **ignore higher-order terms**.\n",
    "* The **first-order term** tells us how to decrease the function:\n",
    "\n",
    "  $$\n",
    "  \\text{Pick } d \\text{ such that } d \\cdot f'(x) < 0\n",
    "  $$\n",
    "* The **best** such direction is:\n",
    "\n",
    "  $$\n",
    "  d = -f'(x)\n",
    "  \\Rightarrow \\text{Gradient Descent}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## 8. 🚀 Final Takeaway: Learning Outcome\n",
    "\n",
    "> **Taylor Series provides a principled justification for why gradient descent updates in the negative gradient direction.**\n",
    "\n",
    "* Shows that local derivative information can help choose a direction where the function value **decreases**.\n",
    "* Gradient descent works by **locally approximating the function** and **moving downhill** based on that approximation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
