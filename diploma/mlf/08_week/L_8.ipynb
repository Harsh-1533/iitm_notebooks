{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b38f4bed",
   "metadata": {},
   "source": [
    "Here are **in-depth notes** for **L8.8: Taylor Series in Higher Dimensions** from the *Foundations of Machine Learning Theory* course, covering **every concept mentioned in the transcript**:\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Lecture Goal:\n",
    "\n",
    "Understand the **Taylor Series** for multivariable functions and its **role in Gradient Descent** and **Constrained Optimization**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 1. **Recap: Taylor Series in 1D**\n",
    "\n",
    "In 1D, the Taylor series for a scalar function $f(x)$ expanded around a point $x$ is:\n",
    "\n",
    "$$\n",
    "f(x + \\eta d) = f(x) + \\eta d f'(x) + \\frac{1}{2} \\eta^2 d^2 f''(x) + \\dots\n",
    "$$\n",
    "\n",
    "* $\\eta$: step size (scalar)\n",
    "* $d$: direction (scalar in 1D)\n",
    "* $f'(x)$: derivative at point $x$\n",
    "* This motivated the use of $-f'(x)$ as a descent direction.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà 2. **Taylor Series in Higher Dimensions**\n",
    "\n",
    "### Goal:\n",
    "\n",
    "Extend the Taylor Series to **vector-valued inputs** $x \\in \\mathbb{R}^d$.\n",
    "\n",
    "$$\n",
    "f(x + \\eta d) \\approx f(x) + \\eta \\cdot d^T \\nabla f(x) + \\text{(higher-order terms)}\n",
    "$$\n",
    "\n",
    "### Key Differences from 1D:\n",
    "\n",
    "* $x, d \\in \\mathbb{R}^d$: vectors\n",
    "* $\\nabla f(x)$: gradient of $f$ at point $x$\n",
    "* $d^T \\nabla f(x)$: **directional derivative**, a scalar\n",
    "\n",
    "üí° This approximation is **first-order**, ignoring higher-order terms (e.g., Hessians).\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ 3. **Dot Product and Directional Derivative**\n",
    "\n",
    "Let:\n",
    "\n",
    "* $a = (a_1, \\dots, a_d)$\n",
    "* $b = (b_1, \\dots, b_d)$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "a^T b = \\sum_{i=1}^{d} a_i b_i\n",
    "$$\n",
    "\n",
    "This gives the **dot product**, which geometrically represents **projection** of one vector onto another and **how aligned they are**.\n",
    "\n",
    "So:\n",
    "\n",
    "* $d^T \\nabla f(x)$ tells us how fast $f$ is increasing in direction $d$\n",
    "* If $d^T \\nabla f(x) < 0$, then $f$ is **decreasing** along $d$\n",
    "* If $d^T \\nabla f(x) > 0$, then $f$ is **increasing** along $d$\n",
    "\n",
    "---\n",
    "\n",
    "## üü¢ 4. **Choosing Descent Directions**\n",
    "\n",
    "We want:\n",
    "\n",
    "$$\n",
    "f(x + \\eta d) - f(x) < 0 \\Rightarrow d^T \\nabla f(x) < 0\n",
    "$$\n",
    "\n",
    "So, any $d$ satisfying:\n",
    "\n",
    "$$\n",
    "d^T \\nabla f(x) < 0\n",
    "$$\n",
    "\n",
    "is a **valid descent direction**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîª 5. **Gradient Descent Direction**\n",
    "\n",
    "Let:\n",
    "\n",
    "$$\n",
    "d = -\\nabla f(x)\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "d^T \\nabla f(x) = -\\|\\nabla f(x)\\|^2 < 0\n",
    "$$\n",
    "\n",
    "‚úÖ Always a valid descent direction\n",
    "‚úÖ Guarantees decrease in function value if $\\eta$ is small enough\n",
    "‚úÖ Always points in direction of **steepest descent**\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ 6. **Geometry of Descent Directions**\n",
    "\n",
    "In 2D, let:\n",
    "\n",
    "* $W = \\nabla f(x)$ (gradient vector)\n",
    "\n",
    "We consider the set of vectors $d \\in \\mathbb{R}^2$ satisfying:\n",
    "\n",
    "* $d^T W = 0$: orthogonal to gradient ‚Üí on a **line**\n",
    "* $d^T W < 0$: descent directions ‚Üí **half-space** below this line\n",
    "* $d^T W > 0$: ascent directions ‚Üí **half-space** above this line\n",
    "\n",
    "Thus, **descent directions** lie in a half-space opposite to gradient.\n",
    "\n",
    "---\n",
    "\n",
    "## üîΩ 7. **Why Gradient Descent?**\n",
    "\n",
    "Although many directions $d$ satisfy $d^T \\nabla f(x) < 0$, only one gives **maximum decrease**:\n",
    "\n",
    "> **The steepest descent direction is $-\\nabla f(x)$**\n",
    "\n",
    "If you're allowed to move by a **unit length**, then:\n",
    "\n",
    "$$\n",
    "\\text{Max decrease in } f(x) \\Rightarrow \\text{Move in direction } -\\nabla f(x)\n",
    "$$\n",
    "\n",
    "üîÅ Thus, gradient descent is also called **steepest descent**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è 8. **Gradient Descent in Constrained Optimization**\n",
    "\n",
    "So far, we've assumed an **unconstrained optimization** problem:\n",
    "\n",
    "$$\n",
    "\\min_x f(x)\n",
    "$$\n",
    "\n",
    "But what if there are constraints?\n",
    "E.g.,\n",
    "\n",
    "$$\n",
    "\\min_x f(x) \\quad \\text{subject to } g(x) \\leq 0\n",
    "$$\n",
    "\n",
    "Now:\n",
    "\n",
    "* Not all directions are valid\n",
    "* If you move in $-\\nabla f(x)$, you might **violate the constraint**\n",
    "\n",
    "Example:\n",
    "In the **cow and rope** example:\n",
    "\n",
    "* Constraint: $g(x) = x_1^2 + x_2^2 - r^2 \\leq 0$\n",
    "* Only points inside the circle (rope radius) are allowed\n",
    "\n",
    "‚ùóIn such cases, pure gradient descent **might fail**, because:\n",
    "\n",
    "* It may suggest a direction that **leads outside the feasible region**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 9. Summary of Key Concepts\n",
    "\n",
    "| Concept                            | Explanation                                                    |\n",
    "| ---------------------------------- | -------------------------------------------------------------- |\n",
    "| **Taylor Series (1D)**             | Approximates function using derivatives                        |\n",
    "| **Taylor Series (Multivariable)**  | $f(x + \\eta d) \\approx f(x) + \\eta d^T \\nabla f(x)$            |\n",
    "| **Directional Derivative**         | $d^T \\nabla f(x)$, measures rate of change along direction $d$ |\n",
    "| **Descent Direction**              | Any $d$ such that $d^T \\nabla f(x) < 0$                        |\n",
    "| **Steepest Descent**               | Direction $-\\nabla f(x)$, gives largest decrease               |\n",
    "| **Gradient Descent**               | Move along $-\\nabla f(x)$ with step size $\\eta$                |\n",
    "| **Constraints Complicate Descent** | With $g(x) \\leq 0$, $-\\nabla f(x)$ may be infeasible           |\n",
    "| **Half-space Geometry**            | Gradient divides space into ascent/descent half-spaces         |\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Next Steps\n",
    "\n",
    "This lecture transitions into **constrained optimization**, where:\n",
    "\n",
    "* Not all directions are allowed\n",
    "* You must consider constraints when picking your descent direction\n",
    "\n",
    "Upcoming topics include:\n",
    "\n",
    "* How to adjust gradient descent for constraints\n",
    "* Karush-Kuhn-Tucker (KKT) conditions\n",
    "* Projected gradient descent\n",
    "* Lagrangian multipliers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
