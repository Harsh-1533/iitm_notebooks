{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb4a25b",
   "metadata": {},
   "source": [
    "Here are **in-depth notes** for **L8.3: Solving an Unconstrained Optimization Problem (Part 1)** from the *Foundations of Machine Learning Theory* course. The lecture focuses on understanding optimization from a computational perspective, starting with a very simple unconstrained example, and introduces the foundational idea behind **gradient descent**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Lecture Focus\n",
    "\n",
    "* How to solve unconstrained optimization problems.\n",
    "* Deriving computational methods (for computers) to solve problems where manual/analytical solutions may not scale.\n",
    "* Establishing the motivation for **iterative gradient-based methods**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© 1. What is an Unconstrained Optimization Problem?\n",
    "\n",
    "### General Optimization Problem:\n",
    "\n",
    "Minimize:\n",
    "\n",
    "$$\n",
    "f(x)\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "* $g_i(x) \\leq 0$ (Inequality constraints)\n",
    "* $h_j(x) = 0$ (Equality constraints)\n",
    "\n",
    "> âœ… **Unconstrained Optimization** means **no constraints** on $x$:\n",
    "\n",
    "$$\n",
    "\\min_{x \\in \\mathbb{R}} f(x)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª 2. Simple Motivating Example\n",
    "\n",
    "### Problem:\n",
    "\n",
    "$$\n",
    "\\min_{x \\in \\mathbb{R}} (x - 5)^2\n",
    "$$\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "* Clearly minimized at $x = 5$\n",
    "* Minimum value = $0$\n",
    "* Reason: Squared function â‰¥ 0 for all real $x$, and becomes 0 only when $x = 5$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® 3. Classical (Analytical) Approach\n",
    "\n",
    "Let $f(x) = (x - 5)^2$\n",
    "\n",
    "### Step-by-step:\n",
    "\n",
    "* Compute derivative:\n",
    "\n",
    "  $$\n",
    "  f'(x) = 2(x - 5)\n",
    "  $$\n",
    "* Set derivative to 0 to find stationary point:\n",
    "\n",
    "  $$\n",
    "  f'(x) = 0 \\Rightarrow x = 5\n",
    "  $$\n",
    "* Confirm itâ€™s a minimum (via second derivative test or function shape)\n",
    "\n",
    "> âš ï¸ This method works for simple functions but **fails** for complex or higher-degree polynomials due to:\n",
    "\n",
    "* Nonlinearities\n",
    "* Higher-order equations that are not easily solvable (e.g., degree 5+)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Example That Breaks the Classical Method\n",
    "\n",
    "$$\n",
    "\\min_{x \\in \\mathbb{R}} 3x^6 + 2x^5 + 3x^3 + 5x^2 + 2\n",
    "$$\n",
    "\n",
    "* First derivative:\n",
    "\n",
    "  $$\n",
    "  f'(x) = 18x^5 + 10x^4 + 9x^2 + 10x\n",
    "  $$\n",
    "* Difficult to solve $f'(x) = 0$ analytically due to:\n",
    "\n",
    "  * Degree 5 polynomial\n",
    "  * No general formula for roots of degree > 4\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ 4. Need for Systematic Computational Procedure\n",
    "\n",
    "We need a method that:\n",
    "\n",
    "* Doesnâ€™t rely on solving high-degree polynomial equations\n",
    "* Is **iterative** and **computer-friendly**\n",
    "* Works for a wide variety of functions\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ€ 5. Iterative Update Idea (Gradient Descent)\n",
    "\n",
    "Start from an arbitrary $x_0 \\in \\mathbb{R}$\n",
    "\n",
    "### Goal:\n",
    "\n",
    "Improve guess $x_t$ over iterations to reach the minimum\n",
    "\n",
    "### General Update Rule:\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t + d\n",
    "$$\n",
    "\n",
    "* $d$ is the **direction of movement**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§­ 6. Direction: What Makes a Good $d$?\n",
    "\n",
    "Letâ€™s look at the function again:\n",
    "\n",
    "$$\n",
    "f(x) = (x - 5)^2\n",
    "$$\n",
    "\n",
    "Plot reveals:\n",
    "\n",
    "* If $x > 5$, move **left** to reduce $f(x)$\n",
    "* If $x < 5$, move **right** to reduce $f(x)$\n",
    "\n",
    "### Therefore:\n",
    "\n",
    "* If $x > 5$, we want $d < 0$\n",
    "* If $x < 5$, we want $d > 0$\n",
    "\n",
    "So $d$ must:\n",
    "\n",
    "* Be a **function of $x$**\n",
    "* Change **direction** depending on whether $x$ is greater or less than 5\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® 7. Using the Derivative to Determine Direction\n",
    "\n",
    "Let:\n",
    "\n",
    "$$\n",
    "f(x) = (x - 5)^2 \\Rightarrow f'(x) = 2(x - 5)\n",
    "$$\n",
    "\n",
    "Observation:\n",
    "\n",
    "* If $x > 5$, $f'(x) > 0$\n",
    "* If $x < 5$, $f'(x) < 0$\n",
    "\n",
    "But we want the **opposite sign** for movement!\n",
    "\n",
    "### So we define:\n",
    "\n",
    "$$\n",
    "d = -f'(x)\n",
    "$$\n",
    "\n",
    "This gives us:\n",
    "\n",
    "* $d < 0$ when $x > 5$ â†’ move left\n",
    "* $d > 0$ when $x < 5$ â†’ move right\n",
    "\n",
    "âœ… Matches desired behavior!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” 8. Iterative Gradient Descent Algorithm (Prototype)\n",
    "\n",
    "### Update Rule:\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t - f'(x_t)\n",
    "$$\n",
    "\n",
    "For $f(x) = (x - 5)^2$, we have:\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t - 2(x_t - 5)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª 9. Example with Bad Behavior (Why We Need Step Size)\n",
    "\n",
    "Start with $x_0 = 10$\n",
    "\n",
    "### First step:\n",
    "\n",
    "$$\n",
    "f'(10) = 2(10 - 5) = 10 \\Rightarrow x_1 = 10 - 10 = 0\n",
    "$$\n",
    "\n",
    "### Second step:\n",
    "\n",
    "$$\n",
    "f'(0) = 2(0 - 5) = -10 \\Rightarrow x_2 = 0 - (-10) = 10\n",
    "$$\n",
    "\n",
    "And this continues:\n",
    "\n",
    "$$\n",
    "x_3 = 0, \\quad x_4 = 10, \\quad x_5 = 0, \\ldots\n",
    "$$\n",
    "\n",
    "â›” **Oscillation!** Never converges to minimum.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš¨ 10. Root Cause: Overshooting the Minimum\n",
    "\n",
    "* Direction is **correct**\n",
    "* But **step is too large**\n",
    "\n",
    "### Insight:\n",
    "\n",
    "We need to **control the magnitude** of the update:\n",
    "\n",
    "* Moving too far = overshooting\n",
    "* Need to take **smaller steps**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”§ 11. Fix: Introduce a Step Size (Learning Rate)\n",
    "\n",
    "Introduce $\\eta > 0$ (small scalar step size)\n",
    "\n",
    "### Modified Update Rule:\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t - \\eta f'(x_t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\eta$ controls how fast/slow we move\n",
    "* Choosing $\\eta$ is crucial:\n",
    "\n",
    "  * Too large: oscillate or diverge\n",
    "  * Too small: converge slowly\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Summary of Key Concepts\n",
    "\n",
    "| Concept                           | Description                                                  |\n",
    "| --------------------------------- | ------------------------------------------------------------ |\n",
    "| **Unconstrained Optimization**    | No constraints; only objective function to minimize          |\n",
    "| **Derivative-based Minimization** | Set $f'(x) = 0$ to find minima (only works for simple cases) |\n",
    "| **Gradient Descent (1D)**         | Use update rule $x_{t+1} = x_t - \\eta f'(x_t)$               |\n",
    "| **Direction**                     | Chosen as negative of gradient to descend function           |\n",
    "| **Problem Without Step Size**     | Oscillates between values; fails to converge                 |\n",
    "| **Solution**                      | Add step size $\\eta$ to control movement                     |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Outcomes Recap\n",
    "\n",
    "* âœ… Understood how to model an unconstrained optimization problem.\n",
    "* âœ… Recognized the limitation of analytic methods for complex functions.\n",
    "* âœ… Learned the principle of gradient descent.\n",
    "* âœ… Saw the necessity of adding a step size to prevent oscillation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
